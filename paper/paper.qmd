---
title: "Forecasting the 2024 U.S. Presidential Election: A Poll-of-Polls Approach for Predicting the Outcome"
subtitle: "Applying Aggregated Poll Data and Statistical Model to Reveal a Narrow Path to Victory in a Highly Competitive Race"
author: 
  - Chendong Fei
  - Xinze Wu
  - Claire Ma
thanks: "Code and data are available at: https://github.com/ke3w/Prediction_US_presidential_election.git"
date: today
date-format: long
abstract: "This paper develops a model to forecast the outcome of the 2024 U.S. presidential election by analyzing aggregated polling data, or “poll-of-polls,” sourced from FiveThirtyEight. Using a generalized linear model, we assess national trends alongside key battleground state polls to predict each candidate's likelihood of victory. The findings indicate a closely contested race, with specific demographic and regional factors creating narrow pathways to winning the presidency. This analysis highlights the value of aggregated polling data in understanding electoral dynamics and demonstrates the importance of statistical modeling in making informed predictions about major political events."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(ggplot2)
library(MASS)
library(dplyr)
```

# Introduction

The outcome of the U.S. presidential election has far-reaching implications, shaping both domestic policies and international relations. As the 2024 election approaches, voters and analysts turn to polls to understand the state of the race between Vice President Kamala Harris, the Democratic candidate, and former President Donald Trump, the Republican candidate. However, individual polls are often limited by their methodologies, timing, and sample demographics, leading to variations in predictions. To overcome these limitations, aggregating multiple polls—a technique known as “poll-of-polls”—provides a more stable and reliable indicator of public opinion. This paper applies a poll-of-polls approach, informed by methodologies from individual polls[@blumenthal2014; @pasek2015], to predict the outcome of the 2024 U.S. presidential election, focusing on data aggregated by FiveThirtyEight[@fivethirtyeight2024].

The primary objective of this analysis is to forecast which candidate is likely to win the 2024 election based on aggregated national and battleground state polling data. By constructing a generalized linear model, we aim to distill insights from the extensive polling data available, examining trends and key demographic indicators.

The primary estimand in this analysis is the probability of each candidate winning the 2024 U.S. presidential election based on aggregated polling data. This probability is derived from a weighted average of poll results across national and battleground states, with adjustments for factors such as recent polling trends, sample sizes, and state-specific electoral significance.

Our analysis reveals a highly competitive race, with key battleground states playing a pivotal role in determining the overall outcome. The model identifies specific regions and demographics that are likely to influence the election results, highlighting the polarized nature of the electorate.As of November 1, 2024, FiveThirtyEight’s national polling average indicates a slight edge for Harris, who has 48.1% support compared to Trump’s 46.7%. Despite this narrow national lead, the race in critical battleground states remains highly competitive. For instance, Pennsylvania is evenly split, with Harris holding marginal leads in states like Wisconsin and Michigan, while Trump shows slight advantages in Nevada, Georgia, and Arizona. These tight margins highlight the crucial role battleground states play in determining the election’s outcome.

These findings underscore the importance of aggregated poll data in capturing the broader political landscape, offering insights that single polls may miss. By understanding the dynamics at play, this study contributes to a broader understanding of electoral processes and the predictive power of statistical models in forecasting complex political events.

This paper is organized as follows: @sec-data discusses the details of the dataset. @sec-Methodology describes the methodology, including generalized linear model. @sec-Results presents the results, highlighting trends in polling, and @sec-Discussion considers the implications of these results for future research on polling and public opinion.

# Data {#sec-data}

## Overview

We use the statistical programming language R [@citeR] to analyze polling data from FiveThirtyEight’s U.S. Presidential election polls [@fivethirtyeight2024]. The dataset contains information such as pollster, polling date, methodology, sample size, state, and candidate support percentages. It allows us to track voter sentiment across different regions and polling methods, providing a comprehensive view of the election landscape. Additionally, irrelevant or incomplete entries were removed to ensure clean, high-quality data, and we retained only key variables to streamline the analysis. This careful selection and cleaning process ensure that the dataset offers a precise and representative snapshot of the election landscape.

## Measurement

The dataset measures public opinion on the 2024 U.S. presidential election by aggregating polling data to estimate voter support for each candidate at both national and state levels. These polling data entries are then aggregated, which applies a weighted adjustment to reflect the reliability, sample size, and recency of each poll. This weighting process addresses the natural variation in polling methodologies (e.g., online survey, phone), sample diversity, and timing, which influence the reliability of each poll as a measure of the broader population’s preferences.For instance, if this poll was conducted a week before the election, it might be weighted more heavily than a poll from three months prior, as it better represents current voter sentiment. By weighting higher-quality and more recent polls more heavily, it creates a comprehensive measure that accounts for both regional and national voter sentiment, smoothing out biases from individual polls.

## Outcome variables

In our analysis, the primary outcome variable is labeled `win`, which is a binary indicator representing the likelihood of a candidate "winning" in each poll based on their support percentage. Specifically, `win` is defined as follows: if a candidate's support percentage (`pct`) in a given poll exceeds 50%, then `win` is assigned a value of 1, indicating a projected win for that candidate in that poll. If the support percentage is 50% or below, `win` is assigned a value of 0, indicating that the candidate is not the likely winner in that poll. This binary outcome variable is particularly useful for logistic regression analysis, as it allows us to model the probability of a candidate achieving majority support in each poll. Using win provides a clear and interpretable framework to assess factors influencing a candidate’s chances of gaining majority support, which aligns well with election forecasting goals. Additionally, this threshold reflects the electoral concept of a "win," as it represents the point at which a candidate has more than half of the vote share, an essential consideration in political analysis.

## Predictor variables

The predictor variables in this analysis were chosen based on their potential influence on polling outcomes and candidate support. Each predictor reflects characteristics of the poll, the pollster, or the candidate's support environment. These variables aim to capture the factors that could impact the likelihood of a candidate reaching majority support (win = 1). Key predictor variables include:

-   **sample_size**: Represents the number of respondents in each poll, with larger sample sizes generally leading to more reliable results.
-   **pollster Rating**: Indicates the quality and historical accuracy of the polling organization, helping to account for differences in poll reliability.
-   **state**: Captures the geographical region of the poll, reflecting regional differences in voter support that are crucial in U.S. elections.

These three variables provide a balanced view of poll quality, reliability, and regional influence, enhancing the model's ability to predict election outcomes accurately. This combination ensures that the model is interpretable and captures essential factors influencing voter sentiment. We apply Bayesian Information Criterion (BIC) method to select significant predictors, and a summary statistics for this method shown in @Appendix

```{r}
# Define the binary response variable if not already defined
# Create a binary `win` variable where pct > 50% indicates a "win"
data <- data %>%
  mutate(win = ifelse(pct > 50, 1, 0))
# Fit the full logistic regression model with all predictors
full_model <- glm(
  win ~ sample_size + pollscore + transparency_score + 
          methodology + start_date + end_date + state + party, 
  data = data, 
  family = binomial
)
# Use stepwise selection based on BIC
best_model <- stepAIC(
  full_model, 
  direction = "both", 
  k = log(nrow(data)) # Set k to log(n) for BIC
)

# Display the summary of the selected model based on BIC
summary(best_model)
# View final model formula and BIC value
formula(best_model)
BIC(best_model)
```

# Model

## Model Analysis
In this study, we used a Generalized Linear Model (GLM) to forecast the winner of the upcoming US presidential election. The response variable was a binary outcome representing whether a candidate was predicted to win (1) or lose (0) in a given state. Given this binary nature of the response variable, we employed a logistic regression with a **logistic link function** to model the probability of a candidate winning a poll.

The model used the following equation:
$$
\log\left(\frac{p_i}{1 - p_i}\right) = \alpha + \beta_1 \cdot \text{pollscore}_i + \beta_2 \cdot \text{sample\_size}_i + \beta_3 \cdot \text{party\_binary}_i + \gamma_{\text{state}_i}
$$
Where:

- \( p_i \) is the probability of a candidate winning a poll in state \(i\).
- \( \alpha \) is the intercept term.
- \( \beta_1, \beta_2, \beta_3 \) are the coefficients for the predictors (`pollscore`, `sample_size`, and `party_binary`), estimated from the model fitting process.
- \( \gamma_{\text{state}_i} \) accounts for state-level effects, as voting preferences can vary significantly across states.

The logistic link function transforms the model's output into the [0, 1] range, which is suitable for representing probabilities.

The predictors we used in the model were:

1. **Poll Score (`pollscore`)**: This variable represents the quality of a poll. Higher scores imply a higher quality, which is expected to result in a more accurate representation of public sentiment.
2. **Sample Size (`sample_size`)**: Larger sample sizes are generally associated with more reliable estimates, reducing the margin of error in poll predictions.
3. **State (`state`)**: Voter preferences often vary significantly by state. Including state-level information allows us to account for regional voting patterns.
4. **Party Affiliation (`party_binary`)**: This binary variable represents the candidate's party, with "Democratic" coded as 1 and "Republican" coded as 0.

## Multicollinearity Check
To check for multicollinearity, we calculated the Variance Inflation Factor (VIF) values for each predictor. The VIF values were as follows:

```plaintext
                 GVIF Df GVIF^(1/(2*Df))
pollscore    1.138990  1        1.067235
sample_size  1.416363  1        1.190110
state        1.622067 54        1.004489
party_binary 1.006902  1        1.003445
```

All VIF values are well below the commonly used threshold of 10, indicating that there is no serious multicollinearity issue among the predictors. This implies that the predictors are not highly correlated and the model coefficients are likely to be stable.

## Model Fit and Diagnostics
To evaluate the quality of the model fit, we performed several diagnostic tests and generated plots for visual assessment.

### Residual Analysis

ROC Curve
We plotted the Receiver Operating Characteristic (ROC) Curve to evaluate the model's classification performance. The area under the curve (AUC) was found to be 0.82, indicating good model discrimination between winners and losers.
```{r}
#| label: fig-roc
#| fig-cap: "ROC Curve for GLM Model"
#| echo: false

library(pROC)
roc_curve <- roc(polls_data_cleaned$win, predict(glm_model, type = "response"))
plot(roc_curve, main = "ROC Curve")
```
## Prediction and Result
The GLM was used to estimate the probability of each candidate winning in each state. Candidates with predicted probabilities greater than 0.5 were classified as winners for that state.

Based on the model, the predicted winner of the US presidential election is the candidate who has the majority of predicted state wins. From the polling data and the model results, Candidate A (e.g., Democratic) was predicted to win the election, securing the majority of states.

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows...

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```

\newpage

# References
